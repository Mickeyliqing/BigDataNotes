vnote_backup_file_826537664 F:/学习文档/大数据/大数据学习笔记/Spark/SparkSQL/SparkSQL_Hive.md
# SparkSQL_Hive

- 代码

```scala
//Scala
object Spark_Sql_Hive {
  /**
    * 启动程序测试之前需要
    * 01：把Hive/conf下的hive-site.xml
    * 02: 把Hadoop/etc/hadoop/conf下的core-site.xml和hdfs-site.xml，复制到Spark/conf文件夹下
    * 03：服务器端需要启动Hadoop,Hive和Spark
    */
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .master("local[2]")
      .appName("Spark_Sql_Hive")
      .config("spark.sql.warehouse.dir", "hdfs://master01:8020/user/hive/warehouse")
      .config("hive.metastore.uris", "thrift://master01:9083")
      .enableHiveSupport()
      .getOrCreate()
    spark.sparkContext.setLogLevel("WARN");
    spark.sql("show tables");
    spark.sql("create table person (id int, name string, age int) row format delimited fields terminated by ' '");
    spark.sql("load data local inpath 'src/main/resources/person.txt' into table person");
    spark.sql("select * from person").show();
    spark.stop();
  }
}
```

```java
//Java
public class Spark_Hive_Demo {
    /**
     * 启动程序测试之前需要
     * 01：把Hive/conf下的hive-site.xml
     * 02: 把Hadoop/etc/hadoop/conf下的core-site.xml和hdfs-site.xml，复制到Spark/conf文件夹下
     * 03：服务器端需要启动Hadoop,Hive和Spark
     * 04：如果是外置的 Hive，第二步可以不操作
     */
    private static final String master = "local[2]";
    private static final String appName = "spark_hive_demo";
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName(appName)
                .master(master)
                .config("spark.sql.warehouse.dir", "hdfs://master01:8020/user/hive/warehouse")
                .config("hive.metastore.uris", "thrift://master01:9083")
                .enableHiveSupport()
                .getOrCreate();
        spark.sparkContext().setLogLevel("WARN");
        spark.sql("show tables");
        spark.sql("create table person (id int, name string, age int) row format delimited fields terminated by ' '");
        spark.sql("load data local inpath 'src/main/resources/person.txt' into table person");
        spark.sql("select * from person").show();
        spark.stop();
    }
```

**注意**

启动程序测试之前需要

- Spark 内置 Hive

1. 把 Hadoop/etc/hadoop/conf 下的 core-site.xml 和 hdfs-site.xml，复制到 Spark/conf 文件夹下。
2. 如果 Spark 路径下发现 metastore_db，需要删除【仅第一次启动的时候】。
3. 第一次启动创建 metastore 的时候，你需要指定 `spark.sql.warehouse.dir` 这个参数，比如：`bin/spark-shell --conf spark.sql.warehouse.dir=hdfs://master01:9000/spark_warehouse`。
4. 注意，如果你在load数据的时候，需要将数据放到HDFS上。

- Spark 外置 Hive

1. 把 Hive/conf 下的 hive-site.xml。
2. 如果 Hive 的 metestore 使用的是 Mysql 数据库，那么需要将 Mysql 的 jdbc 驱动包放到Spark的 jars 目录下。




